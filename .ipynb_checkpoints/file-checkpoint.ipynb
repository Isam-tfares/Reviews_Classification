{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Bibliothiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'joblib' from 'sklearn.externals' (C:\\Users\\hp\\anaconda3\\Lib\\site-packages\\sklearn\\externals\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m shuffle\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstring\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexternals\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m joblib\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stopwords\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'joblib' from 'sklearn.externals' (C:\\Users\\hp\\anaconda3\\Lib\\site-packages\\sklearn\\externals\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from string import punctuation\n",
    "from os import listdir\n",
    "from pickle import dump\n",
    "from sklearn.utils import shuffle\n",
    "import string\n",
    "from sklearn.externals import joblib\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "# import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    file=open(filename,'r')\n",
    "    text=file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "def extract_keywords(text):\n",
    "    tokens = text.split()\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    tokens = [w.lower() for w in tokens if w.isalpha()]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if w not in stop_words]\n",
    "    tokens = [w for w in tokens if len(w) > 1]\n",
    "    return ' '.join(tokens)\n",
    "def Load_All_data(directory):\n",
    "    data=list()\n",
    "    for filename in listdir(directory):\n",
    "        path=directory+'/'+filename\n",
    "\n",
    "        text=extract_keywords(load_data(path))\n",
    "        data.append(text)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "neg_data = Load_All_data('./data/neg/')\n",
    "pos_data = Load_All_data('./data/pos/')\n",
    "\n",
    "# Combine negative and positive data\n",
    "all_data = neg_data + pos_data\n",
    "all_labels = [0] * len(neg_data) + [1] * len(pos_data)\n",
    "\n",
    "# Shuffle the data and labels together\n",
    "shuffled_data, shuffled_labels = shuffle(all_data, all_labels, random_state=42)\n",
    "\n",
    "# Split data into training and testing sets tarin 90%   test 10%\n",
    "train_data,y_train,test_data,y_test = shuffled_data[:900],shuffled_labels[:900],shuffled_data[900:],shuffled_labels[900:]\n",
    "\n",
    "# Convert text data to TF-IDF features\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train = tfidf_vectorizer.fit_transform(train_data)\n",
    "X_test = tfidf_vectorizer.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create and train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.81      0.81       540\n",
      "           1       0.82      0.81      0.81       560\n",
      "\n",
      "    accuracy                           0.81      1100\n",
      "   macro avg       0.81      0.81      0.81      1100\n",
      "weighted avg       0.81      0.81      0.81      1100\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       460\n",
      "           1       1.00      1.00      1.00       440\n",
      "\n",
      "    accuracy                           1.00       900\n",
      "   macro avg       1.00      1.00      1.00       900\n",
      "weighted avg       1.00      1.00      1.00       900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train a Naive Bayes classifier\n",
    "model = SVC()\n",
    "model.fit(X_train, y_train)\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "# Evaluate the model\n",
    "print(classification_report(y_test, y_pred))\n",
    "y_pred2 = model.predict(X_train)\n",
    "# Evaluate the model\n",
    "print(classification_report(y_train, y_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_review(review): \n",
    "    preprocessed_review = extract_keywords(review)  \n",
    "    # Convertir l'avis prétraité en caractéristiques TF-IDF\n",
    "    tfidf_vector = tfidf_vectorizer.transform([preprocessed_review])\n",
    "    # Faire une prédiction avec le modèle\n",
    "    predicted_sentiment = model.predict(tfidf_vector)\n",
    "    # Interpréter la prédiction\n",
    "    if predicted_sentiment[0] == 0:\n",
    "        sentiment_label = \"Négatif\"\n",
    "    else:\n",
    "        sentiment_label = \"Positif\"\n",
    "    print(f\"Votre avis est prédit comme : {sentiment_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Votre avis est prédit comme : Positif\n"
     ]
    }
   ],
   "source": [
    "predict_review(\"it'is amazing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
